import numpy as np
import matplotlib.pyplot as plt

c1 = [0, 1, 2]  # pax_per_min
c2 = [3, 4, 5, 6, 7]
c3 = [8, 9, 10]
np.random.seed(1)

step_number = 60  # 时长60min
episodes = 20000
table = np.zeros([601, 2])  # max_n_state: 10 * 60 + 1 = 601  # 过多了


class Environment:
    """设置Environment"""
    def __init__(self):
        self.waiting_cost = 21  # rmb_per_hour
        self.operation_cost = 50  # rmb_per_trip
        self.step_size = 1  # 1 step == 1 min
        self.step_time = 0  # 0 to T=59

    def reset(self):
        self.step_time = 0
        state = 0
        return state

    @staticmethod
    def arrival_rate():
        p = np.random.rand()
        if p < 0.2:
            a_pax = np.random.choice(c1)
        elif 0.2 <= p <= 0.8:
            a_pax = np.random.choice(c2)
        else:
            a_pax = np.random.choice(c3)
        return a_pax

    def transition(self, state, action):  # 状态转移

        if action == 0:
            next_state = state + self.arrival_rate() * self.step_size
        else:
            next_state = self.arrival_rate() * self.step_size
        return next_state

    """ ? 怎么巧妙地记录下上一时刻的状态，而不用显式地输入参数"""
    def get_reward(self, pre_state, state, action):
        ren_shu = (pre_state + state) * self.step_size / 2  # 梯形面积
        reward1 = - ren_shu * self.waiting_cost / 60  # 乘客等待成本
        reward2 = - action * self.operation_cost  # 发车固定成本

        reward = reward1 + reward2
        return reward  # int

    def step(self, pre_state, state, action):
        next_s = self.transition(state, action)
        reward = self.get_reward(pre_state, state, action)
        if self.step_time <= step_number-1:  # <=59
            done = False
            self.step_time += 1  # 时刻+1
        else:
            done = True
        return next_s, reward, done


class QLearningAgent:
    """设置Agent"""
    def __init__(self):
        self.actions = [0, 1]
        self.epsilon = 0.3
        self.discount_factor = 0.9
        self.learning_rate = 0.01
        self.q_table = table

    def choose_action(self, state):  # behavior_policy: e-greedy
        if np.random.rand() >= self.epsilon:
            action = np.argmax(self.q_table[state])
        else:
            action = np.random.choice(self.actions)
        return action

    def learn(self, state, action, reward, next_state):
        current_q = self.q_table[state, action]
        predict_q = reward + self.discount_factor * max(self.q_table[next_state])  # target_policy: greedy

        self.q_table[state, action] += self.learning_rate * (predict_q - current_q)  # 更新1次


def main():
    env = Environment()
    agent = QLearningAgent()

    episode_reward_list = []
    for i_ep in range(episodes):
        state = env.reset()
        pre_state = 0  # 初值=0
        episode_reward = 0

        done = False
        while not done:  # 到达时刻末,done=True
            action = agent.choose_action(state)
            next_state, reward, done = env.step(pre_state, state, action)
            agent.learn(state, action, reward, next_state)
            pre_state = state  # 保留一次,计算reward用(后面想办法简化去掉)
            state = next_state

            episode_reward += reward
        episode_reward_list.append(episode_reward)
        if i_ep % 2000 == 0:
            agent.epsilon *= 0.8
            print(f'agent.epsilon:{agent.epsilon}')
    return agent.q_table, episode_reward_list


if __name__ == '__main__':
    q_table, reward_list = main()
    print(f'q_table:\n{q_table}')
    print()
    plt.plot(reward_list)
    plt.show()




